version: "3.9"

volumes:
  postgres-explorer-data-100:
  minio-data-100:
  kafka-secrets-100:
  kafka-data-100:
  redis-master-data-100:
  redis-slave-data-100:
  zookeeper-data-100:
  zookeeper-logs-100:
  zookeeper-secrets-100:

networks:
  explorer-infra-net-100:
    external: true

services:
  postgres-explorer:
    restart: unless-stopped
    image: postgres:${POSTGRES_VERSION:-15-alpine}
    shm_size: 500MB
    environment:
      - POSTGRES_LOGGING=true
      - POSTGRES_DB=${POSTGRES_DB:-explorer}
      - POSTGRES_USER=${POSTGRES_USER:-cardano-master}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-dbpass}
    ports:
      - ${POSTGRES_PORT:-54422}:5432
    volumes:
      - postgres-explorer-data-100:/var/lib/postgresql/data
    # command: >
    #   postgres
    #     -c max_connections=250
    #     -c max_locks_per_transaction=256
    #     -c shared_buffers=12GB
    #     -c effective_cache_size=31GB
    #     -c maintenance_work_mem=2GB
    #     -c checkpoint_completion_target=0.9
    #     -c checkpoint_timeout=1h
    #     -c synchronous_commit=off
    #     -c wal_buffers=16MB
    #     -c default_statistics_target=500
    #     -c random_page_cost=1.1
    #     -c effective_io_concurrency=500
    #     -c work_mem=1024GB
    #     -c min_wal_size=1GB
    #     -c max_wal_size=2GB
    #     -c max_parallel_workers_per_gather=4
    #     -c max_parallel_maintenance_workers=4

    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    networks:
      - explorer-infra-net-100
      - default

  minio:
    image: minio/minio:latest
    restart: "no"
    ports:
      - 9200:9000
      - 9201:9091
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY:-minio_access_key}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY:-minio_secret_key}
    volumes:
      - minio-data-100:/data
    command: server /data --console-address ":9091"
    networks:
      - explorer-infra-net-100
      - default

  redis-master:
    restart: unless-stopped
    hostname: cardano.redis.master
    image: redis:7.0.5
    environment:
      - REDIS_REPLICATION_MODE=master
      - REDIS_PASSWORD=${REDIS_MASTER_PASS:-redis_master_pass}
    command: redis-server
    ports:
      - "26416:6379"
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    volumes:
      - redis-master-data-100:/data
    networks:
      - explorer-infra-net-100
      - default

  redis-slave:
    restart: unless-stopped
    hostname: cardano.redis.slave
    image: redis:7.0.5
    environment:
      - REDIS_REPLICATION_MODE=slave
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PASSWORD=${REDIS_MASTER_PASS:-redis_master_pass}
      - REDIS_PASSWORD=${REDIS_SLAVE_PASS:-redis_slave_pass}
    command: redis-server --slaveof redis-master 6379
    ports:
      - "26417:6379"
    links:
      - redis-master
    volumes:
      - redis-slave-data-100:/data
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    networks:
      - explorer-infra-net-100
      - default

  redis-sentinel-100:
    restart: unless-stopped
    hostname: ${REDIS_SENTINEL_HOST:-cardano.redis.sentinel}
    image: "bitnami/redis-sentinel"
    environment:
      - REDIS_MASTER_HOST=cardano.redis.master
      - REDIS_MASTER_PORT_NUMBER=6379
      - REDIS_MASTER_PASSWORD=${REDIS_MASTER_PASS:-redis_master_pass}
      - REDIS_SENTINEL_DOWN_AFTER_MILLISECONDS=5000
      - REDIS_SENTINEL_FAILOVER_TIMEOUT=500
      - REDIS_SENTINEL_QUORUM=2
      - REDIS_SENTINEL_PASSWORD=${REDIS_SENTINEL_PASS:-redis_sentinel_pass}
    ports:
      - "26482:26379"
    depends_on:
      - redis-master
      - redis-slave
    links:
      - redis-master
      - redis-slave
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    networks:
      - explorer-infra-net-100
      - default

  zookeeper:
    restart: unless-stopped
    image: confluentinc/cp-zookeeper:5.1.2
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: "2181"
      ZOOKEEPER_TICK_TIME: "2000"
      ZOOKEEPER_SERVERS: "zookeeper:22888:23888"
    ports:
      - "2284:2181"
    healthcheck:
      test: nc -z localhost 2181 || exit -1
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    volumes:
      - zookeeper-secrets-100:/etc/zookeeper/secrets
      - zookeeper-data-100:/var/lib/zookeeper/data
      - zookeeper-logs-100:/var/lib/zookeeper/log
    networks:
      - explorer-infra-net-100
      - default

  kafka:
    restart: unless-stopped
    image: confluentinc/cp-kafka:7.0.0
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "29198:29092"
      - "29199:29097"
    links:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,EXTERNAL_PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092,EXTERNAL_PLAINTEXT_HOST://172.1.3.178:29098
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: "r1"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_SCHEMA_REGISTRY_URL: "schemaregistry:8085"
      KAFKA_JMX_PORT: 9991
      KAFKA_LOG_RETENTION_BYTES: -1
      KAFKA_LOG_RETENTION_HOURS: -1
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 604800000
      KAFKA_MESSAGE_MAX_BYTES: 199999999
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 199999999
    healthcheck:
      test: nc -z localhost 9092 || exit -1
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    volumes:
      - kafka-secrets-100:/etc/kafka/secrets
      - kafka-data-100:/var/lib/kafka/data
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    networks:
      - explorer-infra-net-100
      - default

  kafdrop:
    image: obsidiandynamics/kafdrop
    restart: "no"
    ports:
      - "9103:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"
    depends_on:
      kafka:
        condition: service_healthy
    links:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "100"
    networks:
      - explorer-infra-net-100
      - default
